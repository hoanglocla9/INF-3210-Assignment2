This repository is based on the methodology introduced in the **SparseGPT** paper. It leverages one-shot pruning techniques to compress massive language models efficiently.

---

## ðŸ“š References

This project is built upon the following research and original source code:

* **[1] Paper:** Frantar, Elias, and Dan Alistarh. *"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot."* International Conference on Machine Learning (ICML). PMLR, 2023.
* **[2] Original Repository:** [IST-DASLab/sparsegpt](https://github.com/IST-DASLab/sparsegpt)
